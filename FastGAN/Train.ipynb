{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta Jupyter Notebook es una adaptación de la red nueronal [implementada](https://github.com/odegeasslbc/FastGAN-pytorch) en el paper [\"Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis\"](https://arxiv.org/pdf/2101.04775.pdf). La particularidad de la misma reside en su capacidad de generar imagenes a partir de datasets \"pequeños\" (>1000 muestras). El código está dividido en tres partes: definición de arquitectura generador/discriminador, funciones auxiliares (procesamiento de datasets, manejo de archivos) y algoritmo de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizaron dos datasets, uno de retinas (Drive) con 160 imagenes en escala de grises y resolución 512x512. El otro consiste de patrones de venas o filamentos, en este caso 6000 imágenes también en escala de grises y resolución 256x256. El procesamiento de ambos casos se hace mediante las clases auxiliares de PyTorch ImageFolder y DataLoader, en batches de 32. Luego se entrena la red con el algoritmo convencional de GAN. Cada cierta cantidad de iteraciones se calcula la FID y se almacena ese valor. Con esto luego se compara en consecuentes iteraciones y se guarda el mejor modelo según esta métrica (menor es mejor). En ambos casos se ve un decenso progresivo de esta distancia, hasta una eventual meseta con sus pequeñas variaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ale\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import spectral_norm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "\n",
    "seq = nn.Sequential\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        try:\n",
    "            m.weight.data.normal_(0.0, 0.02)\n",
    "        except:\n",
    "            pass\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def conv2d(*args, **kwargs):\n",
    "    return spectral_norm(nn.Conv2d(*args, **kwargs))\n",
    "\n",
    "def convTranspose2d(*args, **kwargs):\n",
    "    return spectral_norm(nn.ConvTranspose2d(*args, **kwargs))\n",
    "\n",
    "def batchNorm2d(*args, **kwargs):\n",
    "    return nn.BatchNorm2d(*args, **kwargs)\n",
    "\n",
    "def linear(*args, **kwargs):\n",
    "    return spectral_norm(nn.Linear(*args, **kwargs))\n",
    "\n",
    "class PixelNorm(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input * torch.rsqrt(torch.mean(input ** 2, dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super().__init__()\n",
    "        self.target_shape = shape\n",
    "\n",
    "    def forward(self, feat):\n",
    "        batch = feat.shape[0]\n",
    "        return feat.view(batch, *self.target_shape)        \n",
    "\n",
    "\n",
    "class GLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        nc = x.size(1)\n",
    "        assert nc % 2 == 0, 'channels dont divide 2!'\n",
    "        nc = int(nc/2)\n",
    "        return x[:, :nc] * torch.sigmoid(x[:, nc:])\n",
    "\n",
    "\n",
    "class NoiseInjection(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "\n",
    "    def forward(self, feat, noise=None):\n",
    "        if noise is None:\n",
    "            batch, _, height, width = feat.shape\n",
    "            noise = torch.randn(batch, 1, height, width).to(feat.device)\n",
    "\n",
    "        return feat + self.weight * noise\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, feat):\n",
    "        return feat * torch.sigmoid(feat)\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super().__init__()\n",
    "\n",
    "        self.main = nn.Sequential(  nn.AdaptiveAvgPool2d(4), \n",
    "                                    conv2d(ch_in, ch_out, 4, 1, 0, bias=False), Swish(),\n",
    "                                    conv2d(ch_out, ch_out, 1, 1, 0, bias=False), nn.Sigmoid() )\n",
    "\n",
    "    def forward(self, feat_small, feat_big):\n",
    "        return feat_big * self.main(feat_small)\n",
    "\n",
    "\n",
    "class InitLayer(nn.Module):\n",
    "    def __init__(self, nz, channel):\n",
    "        super().__init__()\n",
    "\n",
    "        self.init = nn.Sequential(\n",
    "                        convTranspose2d(nz, channel*2, 4, 1, 0, bias=False),\n",
    "                        batchNorm2d(channel*2), GLU() )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        noise = noise.view(noise.shape[0], -1, 1, 1)\n",
    "        return self.init(noise)\n",
    "\n",
    "\n",
    "def UpBlock(in_planes, out_planes):\n",
    "    block = nn.Sequential(\n",
    "        nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "        conv2d(in_planes, out_planes*2, 3, 1, 1, bias=False),\n",
    "        #convTranspose2d(in_planes, out_planes*2, 4, 2, 1, bias=False),\n",
    "        batchNorm2d(out_planes*2), GLU())\n",
    "    return block\n",
    "\n",
    "\n",
    "def UpBlockComp(in_planes, out_planes):\n",
    "    block = nn.Sequential(\n",
    "        nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "        conv2d(in_planes, out_planes*2, 3, 1, 1, bias=False),\n",
    "        #convTranspose2d(in_planes, out_planes*2, 4, 2, 1, bias=False),\n",
    "        NoiseInjection(),\n",
    "        batchNorm2d(out_planes*2), GLU(),\n",
    "        conv2d(out_planes, out_planes*2, 3, 1, 1, bias=False),\n",
    "        NoiseInjection(),\n",
    "        batchNorm2d(out_planes*2), GLU()\n",
    "        )\n",
    "    return block\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngf=64, nz=100, nc=3, im_size=1024):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        nfc_multi = {4:16, 8:8, 16:4, 32:2, 64:2, 128:1, 256:0.5, 512:0.25, 1024:0.125}\n",
    "        nfc = {}\n",
    "        for k, v in nfc_multi.items():\n",
    "            nfc[k] = int(v*ngf)\n",
    "\n",
    "        self.im_size = im_size\n",
    "\n",
    "        self.init = InitLayer(nz, channel=nfc[4])\n",
    "                                \n",
    "        self.feat_8   = UpBlockComp(nfc[4], nfc[8])\n",
    "        self.feat_16  = UpBlock(nfc[8], nfc[16])\n",
    "        self.feat_32  = UpBlockComp(nfc[16], nfc[32])\n",
    "        self.feat_64  = UpBlock(nfc[32], nfc[64])\n",
    "        self.feat_128 = UpBlockComp(nfc[64], nfc[128])  \n",
    "        self.feat_256 = UpBlock(nfc[128], nfc[256]) \n",
    "\n",
    "        self.se_64  = SEBlock(nfc[4], nfc[64])\n",
    "        self.se_128 = SEBlock(nfc[8], nfc[128])\n",
    "        self.se_256 = SEBlock(nfc[16], nfc[256])\n",
    "\n",
    "        self.to_128 = conv2d(nfc[128], nc, 1, 1, 0, bias=False) \n",
    "        self.to_big = conv2d(nfc[im_size], nc, 3, 1, 1, bias=False) \n",
    "        \n",
    "        if im_size > 256:\n",
    "            self.feat_512 = UpBlockComp(nfc[256], nfc[512]) \n",
    "            self.se_512 = SEBlock(nfc[32], nfc[512])\n",
    "        if im_size > 512:\n",
    "            self.feat_1024 = UpBlock(nfc[512], nfc[1024])  \n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        feat_4   = self.init(input)\n",
    "        feat_8   = self.feat_8(feat_4)\n",
    "        feat_16  = self.feat_16(feat_8)\n",
    "        feat_32  = self.feat_32(feat_16)\n",
    "\n",
    "        feat_64  = self.se_64( feat_4, self.feat_64(feat_32) )\n",
    "\n",
    "        feat_128 = self.se_128( feat_8, self.feat_128(feat_64) )\n",
    "\n",
    "        feat_256 = self.se_256( feat_16, self.feat_256(feat_128) )\n",
    "\n",
    "        if self.im_size == 256:\n",
    "            return [self.to_big(feat_256), self.to_128(feat_128)]\n",
    "        \n",
    "        feat_512 = self.se_512( feat_32, self.feat_512(feat_256) )\n",
    "        if self.im_size == 512:\n",
    "            return [self.to_big(feat_512), self.to_128(feat_128)]\n",
    "\n",
    "        feat_1024 = self.feat_1024(feat_512)\n",
    "\n",
    "        im_128 = torch.tanh(self.to_128(feat_128))\n",
    "        im_1024 = torch.tanh(self.to_big(feat_1024))\n",
    "\n",
    "        return [im_1024, im_128]\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super(DownBlock, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            conv2d(in_planes, out_planes, 4, 2, 1, bias=False),\n",
    "            batchNorm2d(out_planes), nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "\n",
    "    def forward(self, feat):\n",
    "        return self.main(feat)\n",
    "\n",
    "\n",
    "class DownBlockComp(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super(DownBlockComp, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            conv2d(in_planes, out_planes, 4, 2, 1, bias=False),\n",
    "            batchNorm2d(out_planes), nn.LeakyReLU(0.2, inplace=True),\n",
    "            conv2d(out_planes, out_planes, 3, 1, 1, bias=False),\n",
    "            batchNorm2d(out_planes), nn.LeakyReLU(0.2)\n",
    "            )\n",
    "\n",
    "        self.direct = nn.Sequential(\n",
    "            nn.AvgPool2d(2, 2),\n",
    "            conv2d(in_planes, out_planes, 1, 1, 0, bias=False),\n",
    "            batchNorm2d(out_planes), nn.LeakyReLU(0.2))\n",
    "\n",
    "    def forward(self, feat):\n",
    "        return (self.main(feat) + self.direct(feat)) / 2\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ndf=64, nc=3, im_size=512):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ndf = ndf\n",
    "        self.im_size = im_size\n",
    "\n",
    "        nfc_multi = {4:16, 8:16, 16:8, 32:4, 64:2, 128:1, 256:0.5, 512:0.25, 1024:0.125}\n",
    "        nfc = {}\n",
    "        for k, v in nfc_multi.items():\n",
    "            nfc[k] = int(v*ndf)\n",
    "\n",
    "        if im_size == 1024:\n",
    "            self.down_from_big = nn.Sequential( \n",
    "                                    conv2d(nc, nfc[1024], 4, 2, 1, bias=False),\n",
    "                                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                                    conv2d(nfc[1024], nfc[512], 4, 2, 1, bias=False),\n",
    "                                    batchNorm2d(nfc[512]),\n",
    "                                    nn.LeakyReLU(0.2, inplace=True))\n",
    "        elif im_size == 512:\n",
    "            self.down_from_big = nn.Sequential( \n",
    "                                    conv2d(nc, nfc[512], 4, 2, 1, bias=False),\n",
    "                                    nn.LeakyReLU(0.2, inplace=True) )\n",
    "        elif im_size == 256:\n",
    "            self.down_from_big = nn.Sequential( \n",
    "                                    conv2d(nc, nfc[512], 3, 1, 1, bias=False),\n",
    "                                    nn.LeakyReLU(0.2, inplace=True) )\n",
    "\n",
    "        self.down_4  = DownBlockComp(nfc[512], nfc[256])\n",
    "        self.down_8  = DownBlockComp(nfc[256], nfc[128])\n",
    "        self.down_16 = DownBlockComp(nfc[128], nfc[64])\n",
    "        self.down_32 = DownBlockComp(nfc[64],  nfc[32])\n",
    "        self.down_64 = DownBlockComp(nfc[32],  nfc[16])\n",
    "\n",
    "        self.rf_big = nn.Sequential(\n",
    "                            conv2d(nfc[16] , nfc[8], 1, 1, 0, bias=False),\n",
    "                            batchNorm2d(nfc[8]), nn.LeakyReLU(0.2, inplace=True),\n",
    "                            conv2d(nfc[8], 1, 4, 1, 0, bias=False))\n",
    "\n",
    "        self.se_2_16 = SEBlock(nfc[512], nfc[64])\n",
    "        self.se_4_32 = SEBlock(nfc[256], nfc[32])\n",
    "        self.se_8_64 = SEBlock(nfc[128], nfc[16])\n",
    "        \n",
    "        self.down_from_small = nn.Sequential( \n",
    "                                            conv2d(nc, nfc[256], 4, 2, 1, bias=False), \n",
    "                                            nn.LeakyReLU(0.2, inplace=True),\n",
    "                                            DownBlock(nfc[256],  nfc[128]),\n",
    "                                            DownBlock(nfc[128],  nfc[64]),\n",
    "                                            DownBlock(nfc[64],  nfc[32]), )\n",
    "\n",
    "        self.rf_small = conv2d(nfc[32], 1, 4, 1, 0, bias=False)\n",
    "\n",
    "        self.decoder_big = SimpleDecoder(nfc[16], nc)\n",
    "        self.decoder_part = SimpleDecoder(nfc[32], nc)\n",
    "        self.decoder_small = SimpleDecoder(nfc[32], nc)\n",
    "        \n",
    "    def forward(self, imgs, label, part=None):\n",
    "        if type(imgs) is not list:\n",
    "            imgs = [F.interpolate(imgs, size=self.im_size), F.interpolate(imgs, size=128)]\n",
    "\n",
    "        feat_2 = self.down_from_big(imgs[0])        \n",
    "        feat_4 = self.down_4(feat_2)\n",
    "        feat_8 = self.down_8(feat_4)\n",
    "        \n",
    "        feat_16 = self.down_16(feat_8)\n",
    "        feat_16 = self.se_2_16(feat_2, feat_16)\n",
    "\n",
    "        feat_32 = self.down_32(feat_16)\n",
    "        feat_32 = self.se_4_32(feat_4, feat_32)\n",
    "        \n",
    "        feat_last = self.down_64(feat_32)\n",
    "        feat_last = self.se_8_64(feat_8, feat_last)\n",
    "\n",
    "        #rf_0 = torch.cat([self.rf_big_1(feat_last).view(-1),self.rf_big_2(feat_last).view(-1)])\n",
    "        #rff_big = torch.sigmoid(self.rf_factor_big)\n",
    "        rf_0 = self.rf_big(feat_last).view(-1)\n",
    "\n",
    "        feat_small = self.down_from_small(imgs[1])\n",
    "        #rf_1 = torch.cat([self.rf_small_1(feat_small).view(-1),self.rf_small_2(feat_small).view(-1)])\n",
    "        rf_1 = self.rf_small(feat_small).view(-1)\n",
    "\n",
    "        if label=='real':    \n",
    "            rec_img_big = self.decoder_big(feat_last)\n",
    "            rec_img_small = self.decoder_small(feat_small)\n",
    "\n",
    "            assert part is not None\n",
    "            rec_img_part = None\n",
    "            if part==0:\n",
    "                rec_img_part = self.decoder_part(feat_32[:,:,:8,:8])\n",
    "            if part==1:\n",
    "                rec_img_part = self.decoder_part(feat_32[:,:,:8,8:])\n",
    "            if part==2:\n",
    "                rec_img_part = self.decoder_part(feat_32[:,:,8:,:8])\n",
    "            if part==3:\n",
    "                rec_img_part = self.decoder_part(feat_32[:,:,8:,8:])\n",
    "\n",
    "            return torch.cat([rf_0, rf_1]) , [rec_img_big, rec_img_small, rec_img_part]\n",
    "\n",
    "        return torch.cat([rf_0, rf_1]) \n",
    "\n",
    "\n",
    "class SimpleDecoder(nn.Module):\n",
    "    \"\"\"docstring for CAN_SimpleDecoder\"\"\"\n",
    "    def __init__(self, nfc_in=64, nc=3):\n",
    "        super(SimpleDecoder, self).__init__()\n",
    "\n",
    "        nfc_multi = {4:16, 8:8, 16:4, 32:2, 64:2, 128:1, 256:0.5, 512:0.25, 1024:0.125}\n",
    "        nfc = {}\n",
    "        for k, v in nfc_multi.items():\n",
    "            nfc[k] = int(v*32)\n",
    "\n",
    "        def upBlock(in_planes, out_planes):\n",
    "            block = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "                conv2d(in_planes, out_planes*2, 3, 1, 1, bias=False),\n",
    "                batchNorm2d(out_planes*2), GLU())\n",
    "            return block\n",
    "\n",
    "        self.main = nn.Sequential(  nn.AdaptiveAvgPool2d(8),\n",
    "                                    upBlock(nfc_in, nfc[16]) ,\n",
    "                                    upBlock(nfc[16], nfc[32]),\n",
    "                                    upBlock(nfc[32], nfc[64]),\n",
    "                                    upBlock(nfc[64], nfc[128]),\n",
    "                                    conv2d(nfc[128], nc, 3, 1, 1, bias=False),\n",
    "                                    nn.Tanh() )\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input shape: c x 4 x 4\n",
    "        return self.main(input)\n",
    "\n",
    "from random import randint\n",
    "def random_crop(image, size):\n",
    "    h, w = image.shape[2:]\n",
    "    ch = randint(0, h-size-1)\n",
    "    cw = randint(0, w-size-1)\n",
    "    return image[:,:,ch:ch+size,cw:cw+size]\n",
    "\n",
    "class TextureDiscriminator(nn.Module):\n",
    "    def __init__(self, ndf=64, nc=3, im_size=512):\n",
    "        super(TextureDiscriminator, self).__init__()\n",
    "        self.ndf = ndf\n",
    "        self.im_size = im_size\n",
    "\n",
    "        nfc_multi = {4:16, 8:8, 16:8, 32:4, 64:2, 128:1, 256:0.5, 512:0.25, 1024:0.125}\n",
    "        nfc = {}\n",
    "        for k, v in nfc_multi.items():\n",
    "            nfc[k] = int(v*ndf)\n",
    "\n",
    "        self.down_from_small = nn.Sequential( \n",
    "                                            conv2d(nc, nfc[256], 4, 2, 1, bias=False), \n",
    "                                            nn.LeakyReLU(0.2, inplace=True),\n",
    "                                            DownBlock(nfc[256],  nfc[128]),\n",
    "                                            DownBlock(nfc[128],  nfc[64]),\n",
    "                                            DownBlock(nfc[64],  nfc[32]), )\n",
    "        self.rf_small = nn.Sequential(\n",
    "                            conv2d(nfc[16], 1, 4, 1, 0, bias=False))\n",
    "\n",
    "        self.decoder_small = SimpleDecoder(nfc[32], nc)\n",
    "        \n",
    "    def forward(self, img, label):\n",
    "        img = random_crop(img, size=128)\n",
    "\n",
    "        feat_small = self.down_from_small(img)\n",
    "        rf = self.rf_small(feat_small).view(-1)\n",
    "        \n",
    "        if label=='real':    \n",
    "            rec_img_small = self.decoder_small(feat_small)\n",
    "\n",
    "            return rf, rec_img_small, img\n",
    "\n",
    "        return rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "def InfiniteSampler(n):\n",
    "    \"\"\"Data sampler\"\"\"\n",
    "    i = n - 1\n",
    "    order = np.random.permutation(n)\n",
    "    while True:\n",
    "        yield order[i]\n",
    "        i += 1\n",
    "        if i >= n:\n",
    "            np.random.seed()\n",
    "            order = np.random.permutation(n)\n",
    "            i = 0\n",
    "\n",
    "\n",
    "class InfiniteSamplerWrapper(data.sampler.Sampler):\n",
    "    \"\"\"Data sampler wrapper\"\"\"\n",
    "    def __init__(self, data_source):\n",
    "        self.num_samples = len(data_source)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(InfiniteSampler(self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return 2 ** 31\n",
    "\n",
    "\n",
    "def copy_G_params(model):\n",
    "    flatten = deepcopy(list(p.data for p in model.parameters()))\n",
    "    return flatten\n",
    "    \n",
    "\n",
    "def load_params(model, new_param):\n",
    "    for p, new_p in zip(model.parameters(), new_param):\n",
    "        p.data.copy_(new_p)\n",
    "\n",
    "\n",
    "def get_dir(args):\n",
    "    task_name = 'train_results/' + args.name\n",
    "    saved_model_folder = os.path.join( task_name, 'models')\n",
    "    saved_image_folder = os.path.join( task_name, 'images')\n",
    "    \n",
    "    os.makedirs(saved_model_folder, exist_ok=True)\n",
    "    os.makedirs(saved_image_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "    for f in os.listdir('./'):\n",
    "        if '.py' in f:\n",
    "            shutil.copy(f, task_name+'/'+f)\n",
    "    \n",
    "    with open( os.path.join(saved_model_folder, '../args.txt'), 'w') as f:\n",
    "        json.dump(args.__dict__, f, indent=2)\n",
    "\n",
    "    return saved_model_folder, saved_image_folder\n",
    "\n",
    "\n",
    "class  ImageFolder(Dataset):\n",
    "    \"\"\"docstring for ArtDataset\"\"\"\n",
    "    def __init__(self, root, transform=None):\n",
    "        super( ImageFolder, self).__init__()\n",
    "        self.root = root\n",
    "\n",
    "        self.frame = self._parse_frame()\n",
    "        self.transform = transform\n",
    "\n",
    "    def _parse_frame(self):\n",
    "        frame = []\n",
    "        img_names = os.listdir(self.root)\n",
    "        img_names.sort()\n",
    "        for i in range(len(img_names)):\n",
    "            image_path = os.path.join(self.root, img_names[i])\n",
    "            if image_path[-4:] == '.jpg' or image_path[-4:] == '.png' or image_path[-5:] == '.jpeg': \n",
    "                frame.append(image_path)\n",
    "        return frame\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.frame[idx]\n",
    "        img = Image.open(file).convert('RGB')\n",
    "            \n",
    "        if self.transform:\n",
    "            img = self.transform(img) \n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "\n",
    "from io import BytesIO\n",
    "import lmdb\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MultiResolutionDataset(Dataset):\n",
    "    def __init__(self, path, transform, resolution=256):\n",
    "        self.env = lmdb.open(\n",
    "            path,\n",
    "            max_readers=32,\n",
    "            readonly=True,\n",
    "            lock=False,\n",
    "            readahead=False,\n",
    "            meminit=False,\n",
    "        )\n",
    "\n",
    "        if not self.env:\n",
    "            raise IOError('Cannot open lmdb dataset', path)\n",
    "\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            self.length = int(txn.get('length'.encode('utf-8')).decode('utf-8'))\n",
    "\n",
    "        self.resolution = resolution\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            key = f'{self.resolution}-{str(index).zfill(5)}'.encode('utf-8')\n",
    "            img_bytes = txn.get(key)\n",
    "            #key_asp = f'aspect_ratio-{str(index).zfill(5)}'.encode('utf-8')\n",
    "            #aspect_ratio = float(txn.get(key_asp).decode())\n",
    "\n",
    "        buffer = BytesIO(img_bytes)\n",
    "        img = Image.open(buffer)\n",
    "        img = self.transform(img)\n",
    "\n",
    "        return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:\\backup\\Facultad\\Tesis\\NN\\datasets\\BV\\train\n",
      "{'path': 'f:\\\\backup\\\\Facultad\\\\Tesis\\\\NN\\\\datasets\\\\BV\\\\train', 'cuda': 0, 'name': 'test_bv_256', 'iter': 100000, 'start_iter': 0, 'batch_size': 1, 'im_size': 256, 'ckpt': 'None', 'noise_dim': 400, 'fid_batch_size': 32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ale\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision import utils as vutils\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from scipy import linalg\n",
    "import sys\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import easydict\n",
    "\n",
    "from diffaug import DiffAugment\n",
    "\n",
    "policy = 'color,translation'\n",
    "\n",
    "\n",
    "class FIDScore:\n",
    "    def __init__(self, path_a, path_b, image_size, batch_size, device='cuda'):\n",
    "        self.device = device\n",
    "        self.image_size = image_size\n",
    "        self.path_a = path_a\n",
    "        self.path_b = path_b\n",
    "        self.batch_size = batch_size\n",
    "        self.inception = self.load_patched_inception_v3().eval().to(device)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((self.image_size, self.image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def extract_features(self, loader):\n",
    "        #for batch_idx, batch_data in enumerate(loader):\n",
    "            #print(f\"Batch {batch_idx} - Batch shape: {batch_data.shape}, Batch contents: {batch_data}\")\n",
    "        pbar = tqdm(loader)\n",
    "        feature_list = []\n",
    "        for img in loader:\n",
    "            img = img.to(self.device)\n",
    "            feature = self.inception(img)[0].view(img.shape[0], -1)\n",
    "            feature_list.append(feature.to('cpu'))\n",
    "        features = torch.cat(feature_list, 0)\n",
    "        return features\n",
    "\n",
    "    def calc_fid(self, sample_mean, sample_cov, real_mean, real_cov, eps=1e-6):\n",
    "        cov_sqrt, _ = linalg.sqrtm(sample_cov @ real_cov, disp=False)\n",
    "        if not np.isfinite(cov_sqrt).all():\n",
    "            print('product of cov matrices is singular')\n",
    "            offset = np.eye(sample_cov.shape[0]) * eps\n",
    "            cov_sqrt = linalg.sqrtm((sample_cov + offset) @ (real_cov + offset))\n",
    "        if np.iscomplexobj(cov_sqrt):\n",
    "            if not np.allclose(np.diagonal(cov_sqrt).imag, 0, atol=1e-3):\n",
    "                m = np.max(np.abs(cov_sqrt.imag))\n",
    "                raise ValueError(f'Imaginary component {m}')\n",
    "            cov_sqrt = cov_sqrt.real\n",
    "        mean_diff = sample_mean - real_mean\n",
    "        mean_norm = mean_diff @ mean_diff\n",
    "        trace = np.trace(sample_cov) + np.trace(real_cov) - 2 * np.trace(cov_sqrt)\n",
    "        fid = mean_norm + trace\n",
    "        return fid\n",
    "\n",
    "    def __call__(self, num_workers=4):\n",
    "        dset_a = ImageFolder(self.path_a, self.transform)\n",
    "        print(\"Number of real images:\", len(dset_a))\n",
    "        loader_a = DataLoader(dset_a, batch_size=self.batch_size, num_workers=num_workers, drop_last=True)\n",
    "        features_a = self.extract_features(loader_a).numpy()\n",
    "        real_mean = np.mean(features_a, 0)\n",
    "        real_cov = np.cov(features_a, rowvar=False)\n",
    "\n",
    "        dset_b = ImageFolder(self.path_b, self.transform)\n",
    "        print(\"Number of synthetic images:\", len(dset_b))\n",
    "        loader_b = DataLoader(dset_b, batch_size=self.batch_size, num_workers=num_workers, drop_last=True)\n",
    "        features_b = self.extract_features(loader_b).numpy()\n",
    "        sample_mean = np.mean(features_b, 0)\n",
    "        sample_cov = np.cov(features_b, rowvar=False)\n",
    "\n",
    "        fid = self.calc_fid(sample_mean, sample_cov, real_mean, real_cov)\n",
    "\n",
    "        return fid\n",
    "\n",
    "    @staticmethod\n",
    "    def load_patched_inception_v3():\n",
    "        inception = torch.hub.load('pytorch/vision:v0.9.0', 'inception_v3', pretrained=True)\n",
    "        inception.fc = nn.Identity()\n",
    "        return inception\n",
    "\n",
    "\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def crop_image_by_part(image, part):\n",
    "    hw = image.shape[2] // 2\n",
    "    if part == 0:\n",
    "        return image[:, :, :hw, :hw]\n",
    "    if part == 1:\n",
    "        return image[:, :, :hw, hw:]\n",
    "    if part == 2:\n",
    "        return image[:, :, hw:, :hw]\n",
    "    if part == 3:\n",
    "        return image[:, :, hw:, hw:]\n",
    "\n",
    "\n",
    "def train_d(net, data, label=\"real\"):\n",
    "    \"\"\"Train function of discriminator\"\"\"\n",
    "    if label == \"real\":\n",
    "        part = random.randint(0, 3)\n",
    "        pred, [rec_all, rec_small, rec_part] = net(data, label, part=part)\n",
    "        err = F.relu(torch.rand_like(pred) * 0.2 + 0.8 - pred).mean()\n",
    "        err.backward()\n",
    "        return pred.mean().item(), rec_all, rec_small, rec_part\n",
    "    else:\n",
    "        pred = net(data, label)\n",
    "        err = F.relu(torch.rand_like(pred) * 0.2 + 0.8 + pred).mean()\n",
    "        err.backward()\n",
    "        return pred.mean().item()\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    data_root = args.path\n",
    "    total_iterations = args.iter\n",
    "    checkpoint = args.ckpt\n",
    "    batch_size = args.batch_size\n",
    "    im_size = args.im_size\n",
    "    ndf = 64\n",
    "    ngf = 64\n",
    "    nz = args.noise_dim\n",
    "    nlr = 0.0001\n",
    "    nbeta1 = 0.5\n",
    "    use_cuda = True\n",
    "    multi_gpu = True\n",
    "    dataloader_workers = 8\n",
    "    current_iteration = args.start_iter\n",
    "    save_interval = 100\n",
    "    saved_model_folder, saved_image_folder = get_dir(args)\n",
    "    fid_batch_size = args.fid_batch_size\n",
    "    save_results_dir = args.name\n",
    "    \n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "\n",
    "    transform_list = [\n",
    "        transforms.Resize((int(im_size), int(im_size))),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ]\n",
    "    trans = transforms.Compose(transform_list)\n",
    "\n",
    "    if 'lmdb' in data_root:\n",
    "        from operation import MultiResolutionDataset\n",
    "        dataset = MultiResolutionDataset(data_root, trans, 1024)\n",
    "    else:\n",
    "        dataset = ImageFolder(root=data_root, transform=trans)\n",
    "\n",
    "    dataloader = iter(DataLoader(dataset, batch_size=batch_size, shuffle=False,\n",
    "                                 sampler=InfiniteSamplerWrapper(dataset), num_workers=dataloader_workers,\n",
    "                                 pin_memory=True))\n",
    "    '''\n",
    "    loader = MultiEpochsDataLoader(dataset, batch_size=batch_size, \n",
    "                               shuffle=True, num_workers=dataloader_workers, \n",
    "                               pin_memory=True)\n",
    "    dataloader = CudaDataLoader(loader, 'cuda')\n",
    "    '''\n",
    "\n",
    "    # from model_s import Generator, Discriminator\n",
    "    netG = Generator(ngf=ngf, nz=nz, im_size=im_size)\n",
    "    netG.apply(weights_init)\n",
    "\n",
    "    netD = Discriminator(ndf=ndf, im_size=im_size)\n",
    "    netD.apply(weights_init)\n",
    "\n",
    "    netG.to(device)\n",
    "    netD.to(device)\n",
    "\n",
    "    avg_param_G = copy_G_params(netG)\n",
    "\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=nlr, betas=(nbeta1, 0.999))\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=nlr, betas=(nbeta1, 0.999))\n",
    "    \n",
    "    generated_images = os.path.join(os.getcwd(), 'train_results', save_results_dir, \"images\")    \n",
    "    fid = FIDScore(data_root, generated_images, im_size, fid_batch_size)\n",
    "    fid_scores = []\n",
    "    losses = []\n",
    "    min_fid_score = 999\n",
    "    \n",
    "    data = {\n",
    "            \"loss_d\": [],\n",
    "            \"loss_g\": []\n",
    "        }\n",
    "\n",
    "    loss_results_path = os.path.join(os.getcwd(), 'train_results', save_results_dir, \"loss.json\")      \n",
    "\n",
    "    \n",
    "    if checkpoint != 'None':\n",
    "        ckpt = torch.load(checkpoint)\n",
    "        netG.load_state_dict({k.replace('module.', ''): v for k, v in ckpt['g'].items()})\n",
    "        netD.load_state_dict({k.replace('module.', ''): v for k, v in ckpt['d'].items()})\n",
    "        avg_param_G = ckpt['g_ema']\n",
    "        optimizerG.load_state_dict(ckpt['opt_g'])\n",
    "        optimizerD.load_state_dict(ckpt['opt_d'])\n",
    "        current_iteration = int(checkpoint.split('_')[-1].split('.')[0])\n",
    "        del ckpt\n",
    "\n",
    "    if multi_gpu:\n",
    "        netG = nn.DataParallel(netG.to(device))\n",
    "        netD = nn.DataParallel(netD.to(device))\n",
    "\n",
    "    for iteration in tqdm(range(current_iteration, total_iterations + 1)):\n",
    "        real_image = next(dataloader)\n",
    "        real_image = real_image.to(device)\n",
    "        current_batch_size = real_image.size(0)\n",
    "        noise = torch.Tensor(current_batch_size, nz).normal_(0, 1).to(device)\n",
    "\n",
    "        fake_images = netG(noise)\n",
    "\n",
    "        real_image = DiffAugment(real_image, policy=policy)\n",
    "        fake_images = [DiffAugment(fake, policy=policy) for fake in fake_images]\n",
    "\n",
    "        ## 2. train Discriminator\n",
    "        netD.zero_grad()\n",
    "\n",
    "        err_dr, rec_img_all, rec_img_small, rec_img_part = train_d(netD, real_image, label=\"real\")\n",
    "        train_d(netD, [fi.detach() for fi in fake_images], label=\"fake\")\n",
    "        optimizerD.step()\n",
    "\n",
    "        ## 3. train Generator\n",
    "        netG.zero_grad()\n",
    "        pred_g = netD(fake_images, \"fake\")\n",
    "        err_g = -pred_g.mean()\n",
    "\n",
    "        err_g.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        for p, avg_p in zip(netG.parameters(), avg_param_G):\n",
    "            avg_p.mul_(0.999).add_(0.001 * p.data)\n",
    "\n",
    "            \n",
    "        if iteration % 100 == 0:\n",
    "            print(\"GAN: loss d: %.5f    loss g: %.5f\" % (err_dr, -err_g.item()))\n",
    "            data[\"loss_d\"].append(err_dr)\n",
    "            data[\"loss_g\"].append(-err_g.item())\n",
    "            \n",
    "            with open(loss_results_path, \"w\") as json_file:\n",
    "                json.dump(data, json_file, indent=4)\n",
    "\n",
    "        \n",
    "        if iteration % (save_interval * 10) == 0 or iteration == total_iterations:\n",
    "            backup_para = copy_G_params(netG)\n",
    "            load_params(netG, avg_param_G)\n",
    "            print (\"saving current model\")\n",
    "            torch.save({'g': netG.state_dict(), 'd': netD.state_dict()}, saved_model_folder + '/last_model.pth')\n",
    "            with torch.no_grad():\n",
    "                for i in range (500):\n",
    "                    fixed_noise = torch.FloatTensor(1, nz).normal_(0, 1).to(device)\n",
    "                    generated_images = netG(fixed_noise)                \n",
    "                    #print(\"saved %d image\" % i)\n",
    "                    vutils.save_image(generated_images[0].add(1).mul(0.5), saved_image_folder + '/%d.jpg' % i)\n",
    "\n",
    "            fid_score = fid()\n",
    "            fid_scores.append(fid_score)\n",
    "            print ('Current fid score:' , fid_score)\n",
    "            print ('Fid scores:' , fid_scores)\n",
    "            \n",
    "            fid_results_path = os.path.join(os.getcwd(), 'train_results', save_results_dir, \"fid.txt\")    \n",
    "\n",
    "            with open(fid_results_path, 'w') as file:\n",
    "                for value in fid_scores:\n",
    "                    file.write(str(value) + '\\n')\n",
    "            \n",
    "            if fid_score < min_fid_score:\n",
    "                min_fid_score = fid_score\n",
    "                print('New min fid score: %.12f' % fid_score)\n",
    "                print('Saving best model')\n",
    "                torch.save({'g': netG.state_dict(), 'd': netD.state_dict()}, saved_model_folder + '/min_fid_model.pth')\n",
    "                #load_params(netG, backup_para)\n",
    "                #torch.save({'g': netG.state_dict(), 'd': netD.state_dict(), 'g_ema': avg_param_G, 'opt_g': optimizerG.state_dict(), 'opt_d': optimizerD.state_dict()}, saved_model_folder + '/all_%d.pth' % iteration)\n",
    "\n",
    "            load_params(netG, backup_para)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #data_root = os.path.join(os.path.abspath('..'), \"datasets\", \"drive\", \"img\")\n",
    "    #test_name= 'drive'\n",
    "    #im_size= 512\n",
    "    data_root = os.path.join(os.path.abspath('..'), \"datasets\", \"BV\", \"train\")\n",
    "    test_name= 'test_bv_256'\n",
    "    im_size= 256\n",
    "    print (data_root)\n",
    "    args = easydict.EasyDict({\n",
    "        \"path\": data_root,\n",
    "        \"cuda\": 0,\n",
    "        \"name\": test_name,\n",
    "        \"iter\": 100000,\n",
    "        \"start_iter\": 0, \n",
    "        \"batch_size\": 1,\n",
    "        \"im_size\": im_size,\n",
    "        \"ckpt\": 'None',\n",
    "        \"noise_dim\": 400,\n",
    "        \"fid_batch_size\": 32\n",
    "    })\n",
    "    \n",
    "    print(args)\n",
    "    torch.cuda.empty_cache()\n",
    "    #print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "    train(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
